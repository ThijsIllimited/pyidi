{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "steps_till_convergence = 10\n",
    "n = 13\n",
    "reduce = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indices(n, i, j):\n",
    "    indices_21 = np.ravel_multi_index((i+2, j+1), (n+2,n+2), order='C')\n",
    "    indices_01 = np.ravel_multi_index((i, j+1), (n+2,n+2), order='C')\n",
    "    indices_12 = np.ravel_multi_index((i+1, j+2), (n+2,n+2), order='C')\n",
    "    indices_10 = np.ravel_multi_index((i+1, j), (n+2,n+2), order='C')\n",
    "    indices_11 = np.ravel_multi_index((i+1, j+1), (n+2,n+2), order='C')\n",
    "    indices_00s = np.ravel_multi_index((i, j), (n,n), order='C')\n",
    "    return indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_inverse_numba(Gx, Gy):\n",
    "    Gx2 = np.sum(Gx**2)\n",
    "    Gy2 = np.sum(Gy**2)\n",
    "    GxGy = np.sum(Gx * Gy)\n",
    "    A_inv = 1/(GxGy**2 - Gx2*Gy2) * np.array([[GxGy, -Gx2], [-Gy2, GxGy]])\n",
    "    return A_inv\n",
    "\n",
    "def compute_inverse_numba2(Gx, Gy):\n",
    "    Gx2 = np.sum(Gx**2)\n",
    "    Gy2 = np.sum(Gy**2)\n",
    "    GxGy = np.sum(Gx * Gy)\n",
    "    A_inv = 1/(GxGy**2 - Gx2*Gy2) * np.array([[GxGy, -Gx2], [-Gy2, GxGy]])\n",
    "    return A_inv\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_delta_numba(F, G, Gx, Gy, A_inv):\n",
    "    F_G = G - F\n",
    "    b = np.array([np.sum(Gx*F_G), np.sum(Gy*F_G)])\n",
    "    delta = np.dot(A_inv, b)\n",
    "    error = np.sqrt(np.sum(delta**2))\n",
    "    return delta, error\n",
    "\n",
    "def compute_delta_numba2(F, G, Gx, Gy, A_inv):\n",
    "    F_G = G - F\n",
    "    b = np.array([np.sum(Gx*F_G), np.sum(Gy*F_G)])\n",
    "    delta = np.dot(A_inv, b)\n",
    "    error = np.sqrt(np.sum(delta**2))\n",
    "    return delta, error\n",
    "\n",
    "# ORIGNAL IMPLEMENTATION\n",
    "def original_implementation(A, F):\n",
    "    # Compute the gradients\n",
    "    im1 = A[2:, 1:-1]\n",
    "    im2 = A[:-2, 1:-1]\n",
    "    Gy = (im1 - im2)/2\n",
    "\n",
    "    im1 = A[1:-1, 2:]\n",
    "    im2 = A[1:-1, :-2]\n",
    "    Gx = (im1 - im2)/2\n",
    "\n",
    "    # Compute the inverse of the matrix\n",
    "    A_inv = compute_inverse_numba2(Gx, Gy)\n",
    "    \n",
    "    # Optimize displacment\n",
    "    for i in range(steps_till_convergence):\n",
    "        delta, error = compute_delta_numba2(F, A[1:-1, 1:-1], Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n",
    "# ORIGNAL IMPLEMENTATION with numba\n",
    "@jit(nopython=True)\n",
    "def original_implementation_numba(A, F):\n",
    "    # Compute the gradients\n",
    "    im1 = A[2:, 1:-1]\n",
    "    im2 = A[:-2, 1:-1]\n",
    "    Gy = (im1 - im2)/2\n",
    "\n",
    "    im1 = A[1:-1, 2:]\n",
    "    im2 = A[1:-1, :-2]\n",
    "    Gx = (im1 - im2)/2\n",
    "\n",
    "    # Compute the inverse of the matrix\n",
    "    A_inv = compute_inverse_numba(Gx, Gy)\n",
    "    \n",
    "    # Optimize displacment\n",
    "    for i in range(steps_till_convergence):\n",
    "        delta, error = compute_delta_numba(F, A[1:-1, 1:-1], Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n",
    "# ORIGNAL IMPLEMENTATION flat indexing\n",
    "# Same as above, but with flat indexing instead of slicing. Just in order to compare with the custom windowed implementation\n",
    "@jit(nopython=True) # This is not possible with flat indexing\n",
    "def original_implementation_flat_indexing(A_flat, F_flat, indices_21, indices_01, indices_12, indices_10, indices_11):\n",
    "    im1 = A_flat[indices_21]\n",
    "    im2 = A_flat[indices_01]\n",
    "    Gy = (im1 - im2)/2\n",
    "    im1 = A_flat[indices_12]\n",
    "    im2 = A_flat[indices_10]\n",
    "    Gx = (im1 - im2)/2\n",
    "    A_inv = compute_inverse_numba(Gx, Gy)\n",
    "    A_clipped = A_flat[indices_11]\n",
    "    for i in range(steps_till_convergence):\n",
    "        delta, error = compute_delta_numba(F_flat, A_clipped, Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n",
    "# Custom Windowed Implementation\n",
    "@jit(nopython=True)\n",
    "def custom_windowed_implementation(A_flat, F_flat, indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s):\n",
    "    im1 = A_flat[indices_21]\n",
    "    im2 = A_flat[indices_01]\n",
    "    Gy = (im1 - im2) / 2\n",
    "    im1 = A_flat[indices_12]\n",
    "    im2 = A_flat[indices_10]\n",
    "    Gx = (im1 - im2)/2\n",
    "    A_inv = compute_inverse_numba(Gx, Gy)\n",
    "    A_clipped_new = A_flat[indices_11]\n",
    "    F_clipped_new = F_flat[indices_00s]\n",
    "    for i in range(steps_till_convergence+1):\n",
    "        delta, error = compute_delta_numba(F_clipped_new, A_clipped_new, Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n",
    "# # Custom Windowed Implementation\n",
    "# @jit(nopython=True)\n",
    "# def custom_windowed_implementation(A, F, include):\n",
    "#     im1 = A[2:, 1:-1].ravel()[include.ravel()]\n",
    "#     im2 = A[:-2, 1:-1].ravel()[include.ravel()]\n",
    "#     Gy = (im1 - im2) / 2\n",
    "#     im1 = A[1:-1, 2:].ravel()[include.ravel()]\n",
    "#     im2 = A[1:-1, :-2].ravel()[include.ravel()]\n",
    "#     Gx = (im1 - im2)/2\n",
    "#     A_inv = compute_inverse_numba(Gx, Gy)\n",
    "#     A_clipped_new = A[1:-1, 1:-1].ravel()[include.ravel()]\n",
    "#     F_clipped_new = F.ravel()[include.ravel()]\n",
    "#     for i in range(steps_till_convergence+1):\n",
    "#         delta, error = compute_delta_numba(F_clipped_new, A_clipped_new, Gx, Gy, A_inv)\n",
    "#     return Gy\n",
    "\n",
    "# Custom Windowed Implementation flat\n",
    "def custom_windowed_implementation_flat(A, F, indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s):\n",
    "    im1 = A.flat[indices_21]\n",
    "    im2 = A.flat[indices_01]\n",
    "    Gy = (im1 - im2)/2\n",
    "    im1 = A.flat[indices_12]\n",
    "    im2 = A.flat[indices_10]\n",
    "    Gx = (im1 - im2)/2\n",
    "    A_inv = compute_inverse_numba(Gx, Gy)\n",
    "    A_clipped_new = A.flat[indices_11]\n",
    "    F_clipped_new = F.flat[indices_00s]\n",
    "    for i in range(steps_till_convergence+1):\n",
    "        delta, error = compute_delta_numba(F_clipped_new, A_clipped_new, Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n",
    "def parallel_implementation(A, F, Gy, Gx):\n",
    "    # Compute the gradients\n",
    "    im1 = A[2:, 4:7]\n",
    "    im2 = A[:-2, 4:7]\n",
    "    Gy = (im1 - im2)/2\n",
    "\n",
    "    im1 = A[4:7, 2:]\n",
    "    im2 = A[4:7, :-2]\n",
    "    Gx = (im1 - im2)/2\n",
    "    # Compute the inverse of the matrix\n",
    "    A_inv = compute_inverse_numba(Gx, Gy)\n",
    "    im1 = A[6:9, 1:-1]\n",
    "    im2 = A[4:7, 1:-1]\n",
    "    Gy = (im1 - im2)/2\n",
    "    im1 = A[1:-1, 6:9]\n",
    "    im2 = A[1:-1, 4:7]\n",
    "    Gx = (im1 - im2)/2\n",
    "    A_inv += compute_inverse_numba(Gx, Gy)\n",
    "\n",
    "    # Optimize displacment\n",
    "    for i in range(steps_till_convergence):\n",
    "        delta, error = compute_delta_numba(F, A[1:-1, 1:-1], Gx, Gy, A_inv)\n",
    "    return Gy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
       "        [ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
       "        [ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]]),\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]),\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 12\n",
    "A = np.arange(n)\n",
    "A = A[:, np.newaxis] + A[np.newaxis, :]\n",
    "A, (A[2:, 4:7] - A[:-2, 4:7])/2, (A[1:-1, 2:] - A[1:-1, :-2])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════════════════════════════════════════════════╤══════════════╕\n",
      "│ Comparison                                                        │       Result │\n",
      "╞═══════════════════════════════════════════════════════════════════╪══════════════╡\n",
      "│ Gy(full slicing) == Gy(full flat indexing)                        │  1           │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Gy(full slicing) == Gy(partial flat indexing)                     │  1           │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Gy(full slicing) == Gy(partial flat indexing)                     │  1           │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Time (full window with slicing)                                   │  0.00027672  │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Time (full  window with flat indexing)                            │  0.000113706 │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Time (partial numba)                                              │  5.95689e-05 │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ Time (partial flat indexing)                                      │  4.61176e-05 │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ performance gain % (full slicing vs. partial flat indexing)       │ 78.4732      │\n",
      "├───────────────────────────────────────────────────────────────────┼──────────────┤\n",
      "│ performance gain % (full flat indexing vs. partial flat indexing) │ 47.6117      │\n",
      "╘═══════════════════════════════════════════════════════════════════╧══════════════╛\n"
     ]
    }
   ],
   "source": [
    "n = 13\n",
    "reduce = 2\n",
    "\n",
    "time_full_total = 0\n",
    "time_full_numba_total = 0\n",
    "time_full_flat_total = 0\n",
    "time_partial_total = 0\n",
    "time_partial_flat_total = 0\n",
    "total_steps = 10000\n",
    "\n",
    "include_all = True * np.ones((n,n), dtype=bool)\n",
    "i, j = np.where(include_all)\n",
    "indices_21_all, indices_01_all, indices_12_all, indices_10_all, indices_11_all, indices_00s_all = compute_indices(n, i, j)\n",
    "\n",
    "for it in range(total_steps):\n",
    "    # Generate two frames of random data\n",
    "    A = np.random.randint(0, 255, (n+2, n+2))\n",
    "    F = np.random.randint(0, 255, (n, n))\n",
    "    # Randomly select pixels to exclude from analysis\n",
    "    include = True * np.ones((n,n), dtype=bool)\n",
    "    num_elements = n**2\n",
    "    num_false = n**2 - (n-reduce)**2\n",
    "    # Find the corresponding indices\n",
    "    indices = np.random.choice(num_elements, num_false, replace=False)\n",
    "    include.flat[indices] = False\n",
    "    i, j = np.where(include)\n",
    "    # Set up flat indexing. Each frame the same indexing is used so they are precomputed\n",
    "    indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s = compute_indices(n, i, j)\n",
    "\n",
    "    ### Compare the different L-K implementations\n",
    "    # ORIGNAL IMPLEMENTATION\n",
    "    start_time = time.time()\n",
    "    Gy_ori = original_implementation(A, F)\n",
    "    end_time = time.time()\n",
    "    time_full = end_time - start_time\n",
    "    time_full_total += time_full\n",
    "\n",
    "    # ORIGNAL IMPLEMENTATION with numba\n",
    "    start_time = time.time()\n",
    "    Gy_numba_ori = original_implementation_numba(A, F)\n",
    "    end_time = time.time()\n",
    "    time_numba_full = end_time - start_time\n",
    "    time_full_numba_total += time_numba_full\n",
    "\n",
    "    # # ORIGNAL IMPLEMENTATION flat indexing\n",
    "    A_flat = A.flatten()\n",
    "    start_time = time.time()\n",
    "    Gy_fla = original_implementation_flat_indexing(A_flat, F.flatten(), indices_21_all, indices_01_all, indices_12_all, indices_10_all, indices_11_all)\n",
    "    end_time = time.time()\n",
    "    time_full_flat = end_time - start_time\n",
    "    time_full_flat_total += time_full_flat\n",
    "\n",
    "    # NEW IMPLEMENTATION\n",
    "    start_time = time.time()\n",
    "    Gy_new = custom_windowed_implementation(A_flat, F.flatten(), indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s)\n",
    "    end_time = time.time()\n",
    "    time_partial = end_time - start_time\n",
    "    time_partial_total += time_partial\n",
    "\n",
    "    # # NEW IMPLEMENTATION flat indexing\n",
    "    start_time = time.time()\n",
    "    Gy_new_fla = custom_windowed_implementation_flat(A, F, indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s)\n",
    "    end_time = time.time()\n",
    "    time_partial_flat = end_time - start_time\n",
    "    time_partial_flat_total += time_partial_flat\n",
    "\n",
    "result = np.array_equal(Gy_ori[include], Gy_fla[indices_00s])\n",
    "result2 = np.array_equal(Gy_ori[include], Gy_new)\n",
    "result3 = np.array_equal(Gy_ori[include], Gy_new_fla)\n",
    "\n",
    "table = [[\"Comparison\", \"Result\"],\n",
    "        [\"Gy(full slicing) == Gy(full flat indexing)\", result],\n",
    "        [\"Gy(full slicing) == Gy(partial flat indexing)\", result2],\n",
    "        [\"Gy(full slicing) == Gy(partial flat indexing)\", result3],\n",
    "        [\"Time (full window with slicing)\", time_full_total/total_steps],\n",
    "        [\"Time (full  window with flat indexing)\", time_full_flat_total/total_steps],\n",
    "        [\"Time (partial numba)\", time_partial_total/total_steps],\n",
    "        [\"Time (partial flat indexing)\", time_partial_flat_total/total_steps],\n",
    "        [\"performance gain % (full slicing vs. partial flat indexing)\", (time_full_total-time_partial_total)/time_full_total*100],\n",
    "        [\"performance gain % (full flat indexing vs. partial flat indexing)\", (time_full_flat_total-time_partial_total)/time_full_flat_total*100]]\n",
    "\n",
    "table_str = tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\")\n",
    "print(table_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 5, reduce: 1, time_full_total: 2.6974556446075438e-08, time_full_flat_total: 1.1805367469787598e-09, time_partial_total: 1.096343994140625e-09, time_partial_flat_total: 3.920693397521973e-09\n",
      "n: 5, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.2705421447753907e-09, time_partial_flat_total: 3.726174831390381e-09\n",
      "n: 7, reduce: 1, time_full_total: 2.7381856441497802e-08, time_full_flat_total: 1.0154008865356444e-09, time_partial_total: 9.50791835784912e-10, time_partial_flat_total: 4.415900707244872e-09\n",
      "n: 7, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.3637304306030273e-09, time_partial_flat_total: 3.972392082214355e-09\n",
      "n: 7, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.2747573852539063e-09, time_partial_flat_total: 3.902339935302734e-09\n",
      "n: 9, reduce: 1, time_full_total: 2.716960668563843e-08, time_full_flat_total: 1.1156010627746582e-09, time_partial_total: 1.8156003952026365e-09, time_partial_flat_total: 5.652046203613281e-09\n",
      "n: 9, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.4704084396362305e-09, time_partial_flat_total: 3.768763542175293e-09\n",
      "n: 9, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.1352849006652831e-09, time_partial_flat_total: 3.5509681701660157e-09\n",
      "n: 9, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.3253474235534669e-09, time_partial_flat_total: 3.886394500732422e-09\n",
      "n: 11, reduce: 1, time_full_total: 2.9169065952301026e-08, time_full_flat_total: 1.5059852600097656e-09, time_partial_total: 1.3423681259155272e-09, time_partial_flat_total: 4.320788383483887e-09\n",
      "n: 11, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.048683166503906e-09, time_partial_flat_total: 5.423164367675781e-09\n",
      "n: 11, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.5004158020019533e-09, time_partial_flat_total: 3.887326717376709e-09\n",
      "n: 11, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.3895368576049804e-09, time_partial_flat_total: 4.091732501983642e-09\n",
      "n: 13, reduce: 1, time_full_total: 2.7798793315887448e-08, time_full_flat_total: 1.5573334693908691e-09, time_partial_total: 1.5505123138427732e-09, time_partial_flat_total: 4.586665630340576e-09\n",
      "n: 13, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.630575656890869e-09, time_partial_flat_total: 4.2606663703918455e-09\n",
      "n: 13, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.6704058647155761e-09, time_partial_flat_total: 5.9808611869812016e-09\n",
      "n: 13, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.3103246688842775e-09, time_partial_flat_total: 3.916099071502685e-09\n",
      "n: 15, reduce: 1, time_full_total: 2.7719013690948484e-08, time_full_flat_total: 1.5601205825805664e-09, time_partial_total: 1.7902565002441408e-09, time_partial_flat_total: 5.0976252555847166e-09\n",
      "n: 15, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.851634979248047e-09, time_partial_flat_total: 4.541056156158447e-09\n",
      "n: 15, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.8305635452270506e-09, time_partial_flat_total: 4.1912126541137694e-09\n",
      "n: 15, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.9404911994934083e-09, time_partial_flat_total: 5.474932193756103e-09\n",
      "n: 17, reduce: 1, time_full_total: 2.7047059535980224e-08, time_full_flat_total: 1.9704270362854005e-09, time_partial_total: 1.7403697967529296e-09, time_partial_flat_total: 5.015206336975098e-09\n",
      "n: 17, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.8361997604370116e-09, time_partial_flat_total: 4.857437610626221e-09\n",
      "n: 17, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.7017841339111327e-09, time_partial_flat_total: 4.07142162322998e-09\n",
      "n: 17, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.5042209625244141e-09, time_partial_flat_total: 4.341130256652832e-09\n",
      "n: 19, reduce: 1, time_full_total: 2.81110954284668e-08, time_full_flat_total: 2.0906448364257813e-09, time_partial_total: 2.174229621887207e-09, time_partial_flat_total: 5.541496276855469e-09\n",
      "n: 19, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.0095181465148926e-09, time_partial_flat_total: 5.222387313842774e-09\n",
      "n: 19, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.0709800720214846e-09, time_partial_flat_total: 4.851267337799073e-09\n",
      "n: 19, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.7404913902282713e-09, time_partial_flat_total: 4.83065128326416e-09\n",
      "n: 21, reduce: 1, time_full_total: 2.838444232940674e-08, time_full_flat_total: 2.5682711601257328e-09, time_partial_total: 2.2405099868774415e-09, time_partial_flat_total: 5.995752811431884e-09\n",
      "n: 21, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.2456765174865723e-09, time_partial_flat_total: 5.436520576477051e-09\n",
      "n: 21, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.125577926635742e-09, time_partial_flat_total: 5.095951557159424e-09\n",
      "n: 21, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.1504545211791993e-09, time_partial_flat_total: 4.666266441345215e-09\n",
      "n: 23, reduce: 1, time_full_total: 2.924342393875122e-08, time_full_flat_total: 2.6306009292602542e-09, time_partial_total: 2.6206469535827636e-09, time_partial_flat_total: 6.296930313110351e-09\n",
      "n: 23, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 1.5903663635253907e-09, time_partial_flat_total: 6.101958751678467e-09\n",
      "n: 23, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.2405362129211425e-09, time_partial_flat_total: 5.5147624015808104e-09\n",
      "n: 23, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.2405123710632325e-09, time_partial_flat_total: 5.0160765647888185e-09\n",
      "n: 25, reduce: 1, time_full_total: 2.92812442779541e-08, time_full_flat_total: 2.8507447242736815e-09, time_partial_total: 2.950727939605713e-09, time_partial_flat_total: 7.150652408599853e-09\n",
      "n: 25, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.7959656715393064e-09, time_partial_flat_total: 6.261966228485107e-09\n",
      "n: 25, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.574517726898193e-09, time_partial_flat_total: 5.8109426498413085e-09\n",
      "n: 25, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.5636792182922362e-09, time_partial_flat_total: 5.1837706565856936e-09\n",
      "n: 27, reduce: 1, time_full_total: 3.0031318664550784e-08, time_full_flat_total: 3.3153486251831056e-09, time_partial_total: 3.2241582870483398e-09, time_partial_flat_total: 7.182862758636474e-09\n",
      "n: 27, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.2272315025329593e-09, time_partial_flat_total: 6.868836879730225e-09\n",
      "n: 27, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.9268741607666014e-09, time_partial_flat_total: 6.078217029571533e-09\n",
      "n: 27, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.6705884933471682e-09, time_partial_flat_total: 5.761535167694092e-09\n",
      "n: 29, reduce: 1, time_full_total: 3.0189893245697024e-08, time_full_flat_total: 3.371279239654541e-09, time_partial_total: 3.991210460662842e-09, time_partial_flat_total: 7.616004943847657e-09\n",
      "n: 29, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.2273650169372558e-09, time_partial_flat_total: 7.284836769104004e-09\n",
      "n: 29, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.1008267402648925e-09, time_partial_flat_total: 6.845374107360839e-09\n",
      "n: 29, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 2.918689250946045e-09, time_partial_flat_total: 6.1311674118042e-09\n",
      "n: 31, reduce: 1, time_full_total: 3.0704646110534665e-08, time_full_flat_total: 4.192211627960205e-09, time_partial_total: 4.541280269622803e-09, time_partial_flat_total: 7.87494659423828e-09\n",
      "n: 31, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.730819225311279e-09, time_partial_flat_total: 8.24122667312622e-09\n",
      "n: 31, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.2306718826293947e-09, time_partial_flat_total: 7.181894779205323e-09\n",
      "n: 31, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.0776333808898925e-09, time_partial_flat_total: 6.817867755889893e-09\n",
      "n: 33, reduce: 1, time_full_total: 3.1143088340759275e-08, time_full_flat_total: 4.2539811134338385e-09, time_partial_total: 4.35699462890625e-09, time_partial_flat_total: 9.031188488006591e-09\n",
      "n: 33, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.05099630355835e-09, time_partial_flat_total: 8.417997360229492e-09\n",
      "n: 33, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.876044750213623e-09, time_partial_flat_total: 7.71090030670166e-09\n",
      "n: 33, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.731861114501953e-09, time_partial_flat_total: 1.0245187282562255e-08\n",
      "n: 35, reduce: 1, time_full_total: 3.1990480422973636e-08, time_full_flat_total: 5.232024192810059e-09, time_partial_total: 4.336326122283936e-09, time_partial_flat_total: 9.635779857635499e-09\n",
      "n: 35, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.630823135375976e-09, time_partial_flat_total: 8.863847255706788e-09\n",
      "n: 35, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.0711021423339846e-09, time_partial_flat_total: 8.644213676452636e-09\n",
      "n: 35, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 3.801186084747314e-09, time_partial_flat_total: 7.66829252243042e-09\n",
      "n: 37, reduce: 1, time_full_total: 3.2111082077026366e-08, time_full_flat_total: 5.0649452209472655e-09, time_partial_total: 5.304358005523681e-09, time_partial_flat_total: 1.0233266353607177e-08\n",
      "n: 37, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 5.0386071205139156e-09, time_partial_flat_total: 9.352986812591554e-09\n",
      "n: 37, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.467077255249024e-09, time_partial_flat_total: 9.104738235473633e-09\n",
      "n: 37, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.140496253967285e-09, time_partial_flat_total: 8.197152614593505e-09\n",
      "n: 39, reduce: 1, time_full_total: 3.248987436294555e-08, time_full_flat_total: 5.722084045410156e-09, time_partial_total: 5.586090087890624e-09, time_partial_flat_total: 1.0776264667510986e-08\n",
      "n: 39, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 5.458412170410156e-09, time_partial_flat_total: 1.0440890789031983e-08\n",
      "n: 39, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.8428964614868165e-09, time_partial_flat_total: 9.501368999481201e-09\n",
      "n: 39, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.288067817687989e-09, time_partial_flat_total: 8.92057180404663e-09\n",
      "n: 41, reduce: 1, time_full_total: 3.2877814769744875e-08, time_full_flat_total: 5.655200481414795e-09, time_partial_total: 6.8173789978027345e-09, time_partial_flat_total: 1.1445930004119872e-08\n",
      "n: 41, reduce: 3, time_full_total: [], time_full_flat_total: [], time_partial_total: 5.961720943450928e-09, time_partial_flat_total: 1.1060709953308105e-08\n",
      "n: 41, reduce: 5, time_full_total: [], time_full_flat_total: [], time_partial_total: 5.7191061973571775e-09, time_partial_flat_total: 8.535857200622558e-09\n",
      "n: 41, reduce: 7, time_full_total: [], time_full_flat_total: [], time_partial_total: 4.9059414863586425e-09, time_partial_flat_total: 9.520976543426514e-09\n"
     ]
    }
   ],
   "source": [
    "# def test_performance(function, steps):\n",
    "#     A = np.random.randint(0, 255, (steps,n+2, n+2))\n",
    "#     F = np.random.randint(0, 255, (steps, n, n))\n",
    "#     # Randomly select pixels to exclude from analysis\n",
    "#     include = True * np.ones((n,n), dtype=bool)\n",
    "#     num_elements = n**2\n",
    "#     num_false = n**2 - (n-reduce)**2\n",
    "#     # Find the corresponding indices\n",
    "#     indices = np.random.choice(num_elements, num_false, replace=False)\n",
    "#     include.flat[indices] = False\n",
    "#     i, j = np.where(include)\n",
    "#     # Set up flat indexing. Each frame the same indexing is used so they are precomputed\n",
    "#     indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s = compute_indices(n, i, j)\n",
    "#     time_start = time.time()\n",
    "#     for i in range(steps):\n",
    "#         function(A[i], F[i], indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s)\n",
    "#     time_end = time.time()\n",
    "#     return time_end - time_start\n",
    "\n",
    "LL = 0.15\n",
    "UL = 0.85\n",
    "n_vec = np.arange(5, 42, 2)\n",
    "reduce_vec = np.arange(1, 9, 2)\n",
    "total_steps = 10000\n",
    "computation_time_dict = {}\n",
    "for n in n_vec:\n",
    "    include_all = True * np.ones((n,n), dtype=bool)\n",
    "    i, j = np.where(include_all)\n",
    "    indices_21_all, indices_01_all, indices_12_all, indices_10_all, indices_11_all, indices_00s_all = compute_indices(n, i, j)\n",
    "    for reduce in reduce_vec:\n",
    "        if n-reduce < 2:\n",
    "            continue\n",
    "        time_full_total = []\n",
    "        time_numba_full_total = []\n",
    "        time_full_flat_total = []\n",
    "        time_partial_total = []\n",
    "        time_partial_flat_total = []\n",
    "        for it in range(total_steps):\n",
    "            # Generate two frames of random data\n",
    "            A = np.random.randint(0, 255, (n+2, n+2))\n",
    "            F = np.random.randint(0, 255, (n, n))\n",
    "            # Randomly select pixels to exclude from analysis\n",
    "            include = True * np.ones((n,n), dtype=bool)\n",
    "            num_elements = n**2\n",
    "            num_false = n**2 - (n-reduce)**2\n",
    "            # Find the corresponding indices\n",
    "            indices = np.random.choice(num_elements, num_false, replace=False)\n",
    "            include.flat[indices] = False\n",
    "            i, j = np.where(include)\n",
    "            # Set up flat indexing. Each frame the same indexing is used so they are precomputed\n",
    "            indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s = compute_indices(n, i, j)\n",
    "\n",
    "            ### Compare the different L-K implementations\n",
    "            if reduce == 1:\n",
    "                # ORIGNAL IMPLEMENTATION FULL\n",
    "                start_time = time.time()\n",
    "                Gy_ori = original_implementation(A, F)\n",
    "                end_time = time.time()\n",
    "                time_full = end_time - start_time\n",
    "                \n",
    "                # ORIGNAL IMPLEMENTATION with numba\n",
    "                start_time = time.time()\n",
    "                Gy_numba_ori = original_implementation_numba(A, F)\n",
    "                end_time = time.time()\n",
    "                time_numba_full = end_time - start_time\n",
    "\n",
    "                # # ORIGNAL IMPLEMENTATION flat indexing\n",
    "                A_flat = A.flatten()\n",
    "                start_time = time.time()\n",
    "                Gy_fla = original_implementation_flat_indexing(A_flat, F.flatten(), indices_21_all, indices_01_all, indices_12_all, indices_10_all, indices_11_all)\n",
    "                end_time = time.time()\n",
    "                time_full_flat = end_time - start_time\n",
    "\n",
    "            # NEW IMPLEMENTATION\n",
    "            A_flat = A.flatten()\n",
    "            start_time = time.time()\n",
    "            Gy_new = custom_windowed_implementation(A_flat, F.flatten(), indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s)\n",
    "            end_time = time.time()\n",
    "            time_partial = end_time - start_time\n",
    "\n",
    "            # # NEW IMPLEMENTATION flat indexing\n",
    "            start_time = time.time()\n",
    "            Gy_new_fla = custom_windowed_implementation_flat(A, F, indices_21, indices_01, indices_12, indices_10, indices_11, indices_00s)\n",
    "            end_time = time.time()\n",
    "            time_partial_flat = end_time - start_time\n",
    "\n",
    "            if reduce == 1:\n",
    "                time_full_total.append(time_full)\n",
    "                time_numba_full_total.append(time_numba_full)\n",
    "                time_full_flat_total.append(time_full_flat)\n",
    "            time_partial_total.append(time_partial)\n",
    "            time_partial_flat_total.append(time_partial_flat)\n",
    "        # Convert the lists to numpy arrays\n",
    "        time_full_total = np.array(time_full_total)\n",
    "        time_numba_full_total = np.array(time_numba_full_total)\n",
    "        time_full_flat_total = np.array(time_full_flat_total)\n",
    "        time_partial_total = np.array(time_partial_total)\n",
    "        time_partial_flat_total = np.array(time_partial_flat_total)\n",
    "\n",
    "        # Compute the IQR and remove outliers\n",
    "        if reduce == 1:\n",
    "            time_full_total = np.mean(time_full_total)\n",
    "            time_numba_full_total = np.mean(time_numba_full_total)\n",
    "            time_full_flat_total = np.mean(time_full_flat_total)\n",
    "        time_partial_total = np.mean(time_partial_total)\n",
    "        time_partial_flat_total = np.mean(time_partial_flat_total)\n",
    "\n",
    "        computation_time_dict[(n, reduce)] = [time_full_total/total_steps, time_numba_full_total/total_steps, time_full_flat_total/total_steps, time_partial_total/total_steps, time_partial_flat_total/total_steps]\n",
    "        print(f\"n: {n}, reduce: {reduce}, time_full_total: {time_full_total/total_steps}, time_full_flat_total: {time_full_flat_total/total_steps}, time_partial_total: {time_partial_total/total_steps}, time_partial_flat_total: {time_partial_flat_total/total_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x284743f4350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "ls = ['x', '*', 'o', 'v', '^', '<', '>']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "label_added = False\n",
    "\n",
    "for (n, reduce), value in computation_time_dict.items():\n",
    "    if reduce == 1:\n",
    "        ax.plot(n, value[0], '.r', label='Full - Slicing  - no Numba' if not label_added else '')\n",
    "        ax.plot(n, value[1], '.b', label='Full - Slicing - with Numba' if not label_added else '')\n",
    "        ax.plot(n, value[2], '.c', label='Full - Flat index- with Numba' if not label_added else '')\n",
    "    ax.plot(n, value[3], 'g' + ls[(reduce-1) % len(ls)], label='Partial - Flat Index - with Numba {}'.format(reduce) if not label_added else '')\n",
    "    ax.plot(n, value[4], 'y' + ls[(reduce-1) % len(ls)], label='Partial - Flat Index - no Numba {}'.format(reduce) if not label_added else '')\n",
    "    label_added = True\n",
    "\n",
    "ax.set_xlabel('window size n')\n",
    "ax.set_ylabel('Computation Time of single window of single frame')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "7.77435302734375\n"
     ]
    }
   ],
   "source": [
    "# Create a C-contiguous array of random integers with bit depth 12\n",
    "A = np.random.randint(0, 2**12, size=(1024, 512), dtype=np.int16)\n",
    "print(A.flags['C_CONTIGUOUS'])\n",
    "\n",
    "time_start = time.time()\n",
    "A_fortran = np.asfortranarray(A)\n",
    "time_end = time.time()\n",
    "\n",
    "print(A_fortran.flags['C_CONTIGUOUS'])\n",
    "print((time_end - time_start)*8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
